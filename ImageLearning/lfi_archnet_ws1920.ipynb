{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "lfi-archnet-ws1920.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v9xgSFGXZqdV",
        "colab_type": "text"
      },
      "source": [
        "# Learning from Images, WS19/20\n",
        "### Mahan Ghashghaie, Florian Becker"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pTdKLNoNNrmQ",
        "colab_type": "text"
      },
      "source": [
        "### Was ist zu erledigen?\n",
        "\n",
        "\n",
        "1.   Daten laden\n",
        "2.   Daten augmentieren/transformieren (on-the-fly)\n",
        "3.   Bestehendes ANN als Baseline verwenden\n",
        "4.   Eigenen Ansatz verwenden (z.B. mit den extrahierten Features als Input-Parameter)\n",
        "5.   Evaluation\n",
        "6.   Web-API zur Verfügung stellen (mittels Flask oder Django)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7tdBsGqwkwFQ",
        "colab_type": "text"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D9h92Onliy5n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# PyTorch\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms # Transformations/Augmentations\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torch.utils.data # for DataLoader\n",
        "from torch.autograd import Variable\n",
        "from torchvision import models # pre-trained models\n",
        "\n",
        "# \n",
        "from PIL import Image\n",
        "from skimage import io\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# seaborn\n",
        "import seaborn as sns\n",
        "\n",
        "# NumPy\n",
        "import numpy as np\n",
        "\n",
        "# pandas\n",
        "import pandas as pd\n",
        "\n",
        "# utilities\n",
        "import csv\n",
        "import os\n",
        "import copy\n",
        "\n",
        "print(\"PyTorch Version: \",torch.__version__)\n",
        "print(\"Torchvision Version: \",torchvision.__version__)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vM-QBf5Pk1wd",
        "colab_type": "text"
      },
      "source": [
        "## Access GitLab"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1TzhdU3QjvzK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# this works now, I just had to set the repository to public\n",
        "# otherwise https://techsupportallbugs.wordpress.com/2018/06/05/using-git-with-colab-via-ssh/\n",
        "!git clone -b develop https://gitlab.beuth-hochschule.de/s76343/lfi-ws1920-project"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jsQGJrlrk6n0",
        "colab_type": "text"
      },
      "source": [
        "## Data Preparation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Z2qejFCTzyH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Rename filenames to class_i.JPG\n",
        "# write names and labels to a CSV\n",
        "\n",
        "csv_headers = ['ID', 'image_name', 'label']\n",
        "image_csv_name = 'arc_pictures.csv' \n",
        "root_dir = '/content/lfi-ws1920-project/data/'\n",
        "\n",
        "with open(image_csv_name, 'wt') as f:\n",
        "    csv_writer = csv.writer(f)\n",
        "    csv_writer.writerow(csv_headers) # write header\n",
        "    dirs = ['arcDataset', 'washington_image_db'] # for now both datasets will be mixed and shuffled\n",
        "    \n",
        "    i = 0\n",
        "\n",
        "    for dir in dirs:\n",
        "        print('collecting images for directory', dir)\n",
        "\n",
        "        for style in os.listdir(root_dir + dir):\n",
        "\n",
        "            print('collecting files for style ', style)\n",
        "            style_folder = os.path.join(root_dir + dir, style)\n",
        "\n",
        "            for img in os.listdir(style_folder):\n",
        "                #dst = style + str(i) + \".jpg\"\n",
        "                src =style_folder + '/' + img \n",
        "                #dst =style_folder + '/' + dst \n",
        "                    \n",
        "                # rename() function will rename all the files \n",
        "                #os.rename(src, dst)\n",
        "                csv_writer.writerow([i, src, style])\n",
        "                #csv_writer.writerow([i, dst, style])\n",
        "                i += 1\n",
        "\n",
        "        print(str(i), 'files found')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z8T0Xl_mcIAw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import images from the prepared csv into a DataFrame\n",
        "images_csv = pd.read_csv(image_csv_name)\n",
        "images_csv"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8iH-RGdrcc33",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# show unique classes\n",
        "sorted(images_csv['label'].unique())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QIZNfQuNboK4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# get aggregations and statistics per class\n",
        "images_csv[['label', 'ID']].groupby(['label']).agg(['count'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1XuRuRAyepu-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# aggregate some styles\n",
        "images_csv['label'].replace(['roman'], 'ancient', inplace=True) # do the renaming on the actual frame, not a copy\n",
        "images_csv['label'].replace(['postmodern'], 'modern', inplace=True)\n",
        "images_csv['label'].replace(['international'], 'modern', inplace=True)\n",
        "images_csv['label'].replace(['neo'], 'classicism', inplace=True)\n",
        "\n",
        "sorted(images_csv['label'].unique())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-5Kvgwe1fZFd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# sort out 'classical', 'colonial', 'bauhaus', 'victorian', 'ancient'\n",
        "images_csv = images_csv[~images_csv['label'].isin(['classical', 'colonial', 'bauhaus', 'victorian', 'ancient'])]\n",
        "label_list = sorted(images_csv['label'].unique())\n",
        "label_list"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Wwh1QkZxURa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# remaining number of images\n",
        "num_images = len(images_csv)\n",
        "print('number of images:', num_images)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z_7OY_Wo3u5C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# do a rough 80:20 split train/validation\n",
        "# shuffle the dataframe before assignment\n",
        "images_csv = images_csv.sample(frac=1) # drop=True prevents .reset_index from creating a column containing the old index entries\n",
        "train_upper_bound = int(num_images * .8)\n",
        "images_csv_train = images_csv[0:train_upper_bound] # training\n",
        "images_csv_val = images_csv[train_upper_bound:] # validation\n",
        "print(len(images_csv_train), len(images_csv_val))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DLtgX-iL_Sex",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# get aggregations and statistics per class\n",
        "images_csv[['label', 'ID']].groupby(['label']).agg(['count'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DFA-8W1E1VGB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# calculate weights per class to overcome bias due to imbalancing\n",
        "weights = [2202/366, 2202/447, 2202/370, 2202/257, 2202/191, 2202/443, 2202/456, 2202/58, 2202/165]\n",
        "class_weights = torch.FloatTensor(weights).cuda()\n",
        "class_weights"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Yv4Frt2EjNf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# https://stanford.edu/~shervine/blog/pytorch-how-to-generate-data-parallel\n",
        "# prepare labels for training and validation\n",
        "partition = {'train':images_csv_train['ID'].tolist(), 'val':images_csv_val['ID'].tolist()}\n",
        "\n",
        "train_labels = pd.Series(images_csv_train.label.values, index=images_csv_train.ID).to_dict()\n",
        "val_labels = pd.Series(images_csv_val.label.values, index=images_csv_val.ID).to_dict()\n",
        "labels = dict(train_labels)\n",
        "labels.update(val_labels)\n",
        "\n",
        "label_encoded = dict()\n",
        "label_decoded = dict()\n",
        "for i, label in enumerate(label_list):\n",
        "    label_encoded[i] = label\n",
        "    label_decoded[label] = i\n",
        "\n",
        "# manually add class 'other'\n",
        "#label_encoded[i+1] = 'other'\n",
        "#label_decoded['other'] = i+1\n",
        "\n",
        "print(label_decoded)\n",
        "print(label_encoded)\n",
        "\n",
        "num_classes = len(label_decoded)\n",
        "print('number of classes:', num_classes)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PljbPNsH1Vd5",
        "colab_type": "text"
      },
      "source": [
        "### Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QQU0t91oanHn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "    Define DataLoaders HERE\n",
        "    https://pytorch.org/tutorials/beginner/data_loading_tutorial.html\n",
        "    https://www.kaggle.com/mratsim/starting-kit-for-pytorch-deep-learning\n",
        "    https://gist.github.com/kevinzakka/d33bf8d6c7f06a9d8c76d97a7879f5cb\n",
        "'''\n",
        "\n",
        "class ArchNetDataset(torch.utils.data.Dataset):\n",
        "\n",
        "    def __init__(self, list_ids, labels, transform = None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            data_dir (string): path to image directory\n",
        "        \"\"\"\n",
        "        self.labels = labels\n",
        "        self.list_ids = list_ids\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.list_ids)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        '''\n",
        "            This returns an actual image; will be called by the loader pipeline\n",
        "            returns one sample of the data\n",
        "        '''\n",
        "        ID = self.list_ids[idx] # select sample\n",
        "\n",
        "        # Load data and get label\n",
        "        # es gibt bilder die grayscale sind also shape (batchsize, 1, 128, 128) und damit kommen die conv layer nicht klar\n",
        "        X = Image.open(images_csv.at[ID, 'image_name']).convert('RGB')\n",
        "        # Label müssen in Tensor Form ausgegeben werden (Strings --> kodieren ?)\n",
        "        y = self.labels[ID]\n",
        "        # label als zahl kodieren damit es später als tensor raus kommt und nicht als tuple\n",
        "        # kann später wieder zurückgeführt werden über das andere dict\n",
        "        y = label_decoded[y]\n",
        "        if self.transform:\n",
        "            X = self.transform(X)\n",
        "\n",
        "        return (X, y)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HHk_FKEvl2A4",
        "colab_type": "text"
      },
      "source": [
        "## NN Models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g3NU6GDpZXWf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "    Define NN HERE\n",
        "\n",
        "    input: 3*224*224 image\n",
        "    output: class probabilities (num_classes)\n",
        "\n",
        "    output per layer: (in_size - kernel_size + 2*(padding) / stride) + 1\n",
        "'''\n",
        "# ArchNet Neural Network\n",
        "class ArchNet(nn.Module):\n",
        "\n",
        "    def __init__(self, num_channels = 3, num_classes = num_classes, print_shapes = False):\n",
        "        super(ArchNet, self).__init__()\n",
        "        self.kernel_size = 5\n",
        "        self.stride = 1\n",
        "        self.padding = 1\n",
        "        self.print_shapes = print_shapes\n",
        "\n",
        "        self.conv1 = nn.Sequential(\n",
        "            nn.Conv2d(num_channels, 96, 7, self.stride, self.padding),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.BatchNorm2d(96),\n",
        "            nn.MaxPool2d(2))\n",
        "        self.conv2 = nn.Sequential(\n",
        "            nn.Conv2d(96, 256, 5, 2, self.padding),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.MaxPool2d(3))\n",
        "        self.conv3 = nn.Sequential(\n",
        "            nn.Conv2d(256, 384, 3, 2, self.padding),\n",
        "            nn.ReLU(inplace=True),\n",
        "            #nn.MaxPool2d(3)\n",
        "            )\n",
        "        self.conv4 = nn.Sequential(\n",
        "            nn.Conv2d(384, 384, 3, 2, self.padding),\n",
        "            nn.ReLU(inplace=True),\n",
        "            #nn.MaxPool2d(3)\n",
        "            )\n",
        "        self.norm1 = nn.BatchNorm2d(384)\n",
        "        self.drop_out = nn.Dropout()\n",
        "        self.fc1 = nn.Linear(384 * 5 * 5, 512)\n",
        "        self.fc2 = nn.Linear(512, 1000)\n",
        "        self.classifier = nn.Linear(1000, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        ## Expected Dimension Error kommt hier weil der Input geflattened wird, die Conv-Layer aber ein Bild, 4 Dimensionen erwarten\n",
        "        ## also die shape die auch unten bei local_batch ausgegeben wird.\n",
        "        #print('input shape', x.shape)\n",
        "\n",
        "        # Layer 1\n",
        "        x = self.conv1(x)\n",
        "        if self.print_shapes == True:\n",
        "            print('after layer1', x.shape)\n",
        "\n",
        "        # Layer 2\n",
        "        x = self.conv2(x)\n",
        "        if self.print_shapes == True:\n",
        "            print('after layer2', x.shape)\n",
        "\n",
        "        # Layer 3\n",
        "        x = self.conv3(x)\n",
        "        if self.print_shapes == True:\n",
        "            print('after layer3', x.shape)\n",
        "\n",
        "        x = self.conv4(x)\n",
        "        if self.print_shapes == True:\n",
        "            print('after layer3.5', x.shape)\n",
        "\n",
        "        # Layer 4\n",
        "        x = x.reshape(x.size(0), -1)\n",
        "        x = self.drop_out(x)\n",
        "        if self.print_shapes == True:\n",
        "            print('after drop_out', x.shape)\n",
        "\n",
        "        # Layer 5\n",
        "        #x = x.view(x.size(0), -1)\n",
        "        x = self.fc1(x)\n",
        "        if self.print_shapes == True:\n",
        "            print('after fc1', x.shape)\n",
        "\n",
        "\n",
        "        # Layer 6\n",
        "        x = self.fc2(x)\n",
        "        if self.print_shapes == True:\n",
        "            print('after fc2', x.shape)\n",
        "\n",
        "        # linear layer anpassen an den geflatteten tensor\n",
        "        #x = x.view(x.size(0), -1)\n",
        "        # Classifier\n",
        "        x = self.classifier(x)\n",
        "\n",
        "        return x\n",
        "'''\n",
        "class ArchNetV2(nn.Module):\n",
        "    def __init__(self, num_channels = 3, num_classes = num_classes, print_shapes = False):\n",
        "        super(ArchNetV2, self).__init__()\n",
        "        self.print_shapes = print_shapes\n",
        "\n",
        "        self.conv1 = nn.Sequential(\n",
        "            nn.Conv2d(num_channels, 96, 7, 3),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(2),\n",
        "            nn.BatchNorm2d(96)\n",
        "        )\n",
        "        self.conv2 = nn.Sequential(\n",
        "            nn.Conv2d(96, 256, 5, 2),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(2),\n",
        "            nn.BatchNorm2d(256)\n",
        "        )\n",
        "        self.conv3 = nn.Sequential(\n",
        "            nn.Conv2d(256, 384, 5, stride=1),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "        self.conv4 = nn.Sequential(\n",
        "            nn.Conv2d(384, 384, 3, stride=1),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "        self.fc1 = nn.Linear(384 * 2 * 2, 1000)\n",
        "        self.drop_out = nn.Dropout()\n",
        "        self.fc2 = nn.Linear(1000, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        # Layer 1\n",
        "        x = self.conv1(x)\n",
        "        if self.print_shapes == True:\n",
        "            print('after layer1', x.shape)\n",
        "\n",
        "        # Layer 2\n",
        "        x = self.conv2(x)\n",
        "        if self.print_shapes == True:\n",
        "            print('after layer2', x.shape)\n",
        "\n",
        "        # Layer 3\n",
        "        x = self.conv3(x)\n",
        "        if self.print_shapes == True:\n",
        "            print('after layer3', x.shape)\n",
        "\n",
        "        # Layer 4\n",
        "        x = self.conv4(x)\n",
        "        if self.print_shapes == True:\n",
        "            print('after layer4', x.shape)\n",
        "\n",
        "        # Layer 5\n",
        "        x = self.fc1(x)\n",
        "        if self.print_shapes == True:\n",
        "            print('after layer5', x.shape)\n",
        "\n",
        "        # Layer 6\n",
        "        x = x.reshape(x.size(0), -1)\n",
        "        x = self.drop_out(x)\n",
        "        if self.print_shapes == True:\n",
        "            print('after layer6', x.shape)\n",
        "\n",
        "        # linear layer anpassen an den geflatteten tensor\n",
        "        #x = x.view(x.size(0), -1)\n",
        "        # Classifier\n",
        "        x = self.fc2(x)\n",
        "\n",
        "        return x\n",
        "'''        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PhXnvCOc9K15",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def set_parameter_requires_grad(model, feature_extracting):\n",
        "    if feature_extracting:\n",
        "        for param in model.parameters():\n",
        "            param.requires_grad = False"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AC9LlsQpaQL4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "    Setup run parameters here & initialise NN\n",
        "'''\n",
        "model = 'resnet' # TODO\n",
        "\n",
        "# CUDA for PyTorch\n",
        "use_cuda = torch.cuda.is_available()\n",
        "\n",
        "# Parameters\n",
        "batch_size = 16 # how many samples per batch to load\n",
        "\n",
        "# each worker works on its own batch; should be decreased (i.e. set to 0) if system resources are running out\n",
        "num_workers = 1 if use_cuda else 2\n",
        "pin_memory = True if use_cuda else False # whether to copy tensors into CUDA pinned memory\n",
        "n_epochs_train = 50\n",
        "n_epocs_val = 25\n",
        "\n",
        "# Flag for feature extracting. When False, we finetune the whole model,\n",
        "#   when True we only update the reshaped layer params\n",
        "feature_extract = False\n",
        "\n",
        "if model == 'archnet':\n",
        "    model = ArchNet(num_classes=num_classes, num_channels=3, print_shapes=False)\n",
        "    #model = ArchNetV2(num_classes=num_classes, num_channels=3, print_shapes=True)\n",
        "    set_parameter_requires_grad(model, False)\n",
        "    num_ftrs = model.classifier.in_features\n",
        "    model.classifier = nn.Linear(num_ftrs, num_classes)\n",
        "    #This will normalize the image in the range [-1,1]. \n",
        "    #For example, the minimum value 0 will be converted to (0-0.5)/0.5=-1, the maximum value of 1 will be converted to (1-0.5)/0.5=1.    \n",
        "    #normalize = transforms.Normalize(mean=[.5, .5, .5], std=[.5, .5, .5])\n",
        "    normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "    input_size = 224 # TODO test\n",
        "\n",
        "elif model == 'resnet':\n",
        "    # https://pytorch.org/tutorials/beginner/finetuning_torchvision_models_tutorial.html\n",
        "    model = models.resnet50(pretrained=True)\n",
        "    set_parameter_requires_grad(model, feature_extract)\n",
        "    num_ftrs = model.fc.in_features\n",
        "    model.fc = nn.Linear(num_ftrs, num_classes)\n",
        "    normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]) # expected Normalization values for ResNet\n",
        "    input_size = 224 # expected value for ResNet\n",
        "    n_epochs_train = 25\n",
        "    n_epochs_val = 25\n",
        "\n",
        "elif model == 'vgg':\n",
        "    model = models.vgg16(pretrained=True)\n",
        "    set_parameter_requires_grad(model, feature_extract)\n",
        "    num_ftrs = model.classifier[6].in_features\n",
        "    model.classifier[6] = nn.Linear(num_ftrs, num_classes)\n",
        "    normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]) # expected Normalization values for ResNet\n",
        "    input_size = 224 # expected value for ResNet\n",
        "    n_epochs_train = 50\n",
        "    n_epochs_val = 25\n",
        "\n",
        "elif model == \"densenet\":\n",
        "    model = models.densenet121(pretrained=True)\n",
        "    set_parameter_requires_grad(model, feature_extract)\n",
        "    num_ftrs = model.classifier.in_features\n",
        "    model.classifier = nn.Linear(num_ftrs, num_classes)\n",
        "    normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "    input_size = 224\n",
        "    n_epochs_train = 25\n",
        "\n",
        "elif model == 'googlenet':\n",
        "    model = models.googlenet(pretrained=True)\n",
        "    set_parameter_requires_grad(model, feature_extract)\n",
        "    num_ftrs = model.fc.in_features\n",
        "    model.fc = nn.Linear(num_ftrs, num_classes)\n",
        "    normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "    input_size = 224\n",
        "    n_epochs_train = 25\n",
        "\n",
        "elif model == 'squeezenet':\n",
        "    model = models.squeezenet1_0(pretrained=True)\n",
        "    set_parameter_requires_grad(model, feature_extract)\n",
        "    model.classifier[1] = nn.Conv2d(512, num_classes, kernel_size=(1,1), stride=(1,1))\n",
        "    model.num_classes = num_classes\n",
        "    normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "    input_size = 224\n",
        "    n_epochs_train = 50\n",
        "    n_epochs_val = 25\n",
        "\n",
        "elif model == 'mobilenet':\n",
        "    model = models.mobilenet_v2(pretrained=True)\n",
        "    set_parameter_requires_grad(model, feature_extract)\n",
        "    num_ftrs = model.classifier[1].in_features\n",
        "    model.classifier[1] = nn.Linear(num_ftrs, num_classes)\n",
        "    normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "    input_size = 224\n",
        "    n_epochs_train = 25\n",
        "    n_epochs_val = 25\n",
        "\n",
        "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
        "model = model.to(device) # send model to either GPU or CPU\n",
        "\n",
        "#if feature_extract == True:\n",
        "#    for param in model.parameters():\n",
        "#        param.requires_grad = False\n",
        "\n",
        "params_to_update = model.parameters()\n",
        "print(\"Params to learn:\")\n",
        "if feature_extract:\n",
        "    params_to_update = []\n",
        "    for name,param in model.named_parameters():\n",
        "        if param.requires_grad == True:\n",
        "            params_to_update.append(param)\n",
        "            print(\"\\t\",name)\n",
        "else:\n",
        "    for name,param in model.named_parameters():\n",
        "        if param.requires_grad == True:\n",
        "            print(\"\\t\",name)\n",
        "\n",
        "optimizer = optim.SGD(params_to_update, lr=0.001, momentum=0.9)\n",
        "criterion = nn.CrossEntropyLoss(weight=class_weights, reduction='mean')\n",
        "if use_cuda == True:\n",
        "    model.cuda() # send model to the GPU\n",
        "    criterion.cuda() # send optimizer to GPU\n",
        "\n",
        "# for debugging\n",
        "#torch.autograd.set_detect_anomaly(True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c9VGB4bOzUhs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(model)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gH7g-mELkVPR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "    Define Tranformations here\n",
        "'''\n",
        "\n",
        "train_transform = transforms.Compose([\n",
        "    # you can add other transformations in this list\n",
        "    transforms.Resize((input_size, input_size)), # Resize the image to 256x256\n",
        "    #transforms.RandomResizedCrop(input_size), # Crop the image to H×W pixels about the center.\n",
        "    transforms.RandomHorizontalFlip(), # flip image horizontally\n",
        "    transforms.RandomRotation((-15, 15)), # apply random rotation of +-10°\n",
        "    transforms.RandomPerspective(),\n",
        "    transforms.RandomCrop(input_size),\n",
        "    #transforms.RandomAffine((-5, 5)),\n",
        "    #transforms.RandomVerticalFlip(),\n",
        "    #transforms.Grayscale(num_output_channels=1),\n",
        "    transforms.ToTensor(),\n",
        "    normalize\n",
        "])\n",
        "print(train_transform)\n",
        "\n",
        "valid_transform = transforms.Compose([\n",
        "    transforms.Resize((input_size, input_size)), # Resize the image to 256x256\n",
        "    #transforms.RandomHorizontalFlip(),\n",
        "    transforms.CenterCrop(input_size),\n",
        "    #transforms.Grayscale(num_output_channels=1),                                  \n",
        "    transforms.ToTensor(),\n",
        "    normalize                             \n",
        "])\n",
        "print(valid_transform)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0TV6s44oylj9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Generators\n",
        "batch_size = 1 # TODO\n",
        "training_set = ArchNetDataset(partition['train'], labels, transform=train_transform)\n",
        "training_generator = torch.utils.data.DataLoader(training_set, batch_size=4, num_workers=num_workers, shuffle=True, pin_memory=pin_memory)\n",
        "\n",
        "valid_set = ArchNetDataset(partition['val'], labels, transform=valid_transform)\n",
        "valid_loader = torch.utils.data.DataLoader(valid_set, batch_size=batch_size, num_workers=num_workers, shuffle=True, pin_memory=pin_memory)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qjCG6aqBWlBV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "    Let’s visualize a few training images so as to understand the data augmentations.\n",
        "'''\n",
        "\n",
        "def imshow(inp, title=None):\n",
        "    \"\"\"Imshow for Tensor.\"\"\"\n",
        "    inp = inp.numpy().transpose((1, 2, 0))\n",
        "    mean = np.array([0.485, 0.456, 0.406])\n",
        "    std = np.array([0.229, 0.224, 0.225])\n",
        "    inp = std * inp + mean\n",
        "    inp = np.clip(inp, 0, 1)\n",
        "    plt.figure(figsize = (25,3))\n",
        "    plt.imshow(inp)\n",
        "    if title is not None:\n",
        "        plt.title(title)\n",
        "    plt.pause(0.001)  # pause a bit so that plots are updated\n",
        "\n",
        "\n",
        "# Get a batch of training data\n",
        "inputs, classes = next(iter(training_generator))\n",
        "\n",
        "# Make a grid from batch\n",
        "out = torchvision.utils.make_grid(inputs)\n",
        "imshow(out, title=[label_encoded[int(x)] for x in classes])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lAxyHUCfl_8W",
        "colab_type": "text"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JQ1sGT_jmCA8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%time\n",
        "\n",
        "model.train() # set to TRAIN mode\n",
        "\n",
        "for epoch in range(n_epochs_train):\n",
        "    \n",
        "    print('Epoch {}/{}'.format(epoch, n_epochs_train - 1))\n",
        "    print('-' * 10)\n",
        "\n",
        "    running_loss = 0.0\n",
        "    running_corrects = 0\n",
        "    \n",
        "    for local_batch, local_labels in training_generator:\n",
        "        # Transfer to GPU\n",
        "        local_batch = Variable(local_batch)\n",
        "        #print(local_labels)\n",
        "        local_labels = Variable(local_labels)\n",
        "        \n",
        "        # zero parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        if use_cuda == True:\n",
        "            local_batch = local_batch.cuda()\n",
        "            local_labels = local_labels.cuda()\n",
        "\n",
        "        # Forward + Backward + Optimize\n",
        "        outputs = model(local_batch)\n",
        "        loss = criterion(outputs, local_labels)\n",
        "\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "\n",
        "        #print('Epoch {}, train Loss: {:.3f}'.format(epoch, loss.item()))\n",
        "        loss.backward() # computes the gradients\n",
        "        optimizer.step() # adapt the weights\n",
        "\n",
        "        # statistics\n",
        "        running_loss += loss.item() * local_batch.size(0)\n",
        "        running_corrects += torch.sum(preds == local_labels.data)\n",
        "\n",
        "    epoch_loss = running_loss / len(training_generator.dataset)\n",
        "    epoch_acc = running_corrects.double() / len(training_generator.dataset)\n",
        "\n",
        "    print('{} Loss: {:.4f} Acc: {:.4f}'.format('TRAIN', epoch_loss, epoch_acc))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b5JMgvKFRprc",
        "colab_type": "text"
      },
      "source": [
        "## VALIDATION"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "24xIPIWhRj56",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%time\n",
        "\n",
        "model.eval() # set to VALIDATION mode\n",
        "\n",
        "val_acc_history = []\n",
        "\n",
        "#best_model_wts = copy.deepcopy(model.state_dict())\n",
        "best_acc = 0.0\n",
        "n_epochs = 50\n",
        "\n",
        "for epoch in range(n_epochs_val):\n",
        "    \n",
        "    print('Epoch {}/{}'.format(epoch, n_epochs_val - 1))\n",
        "    print('-' * 10)\n",
        "\n",
        "    running_loss = 0.0\n",
        "    running_corrects = 0\n",
        "    \n",
        "    for local_batch, local_labels in valid_loader:\n",
        "        # Transfer to GPU\n",
        "        local_batch = Variable(local_batch)\n",
        "        #print(local_labels)\n",
        "        local_labels = Variable(local_labels)\n",
        "        \n",
        "        # zero parameter gradients\n",
        "        #optimizer.zero_grad()\n",
        "\n",
        "        if use_cuda == True:\n",
        "            local_batch = local_batch.cuda()\n",
        "            local_labels = local_labels.cuda()\n",
        "\n",
        "        # Forward + Backward + Optimize\n",
        "        outputs = model(local_batch)\n",
        "        loss = criterion(outputs, local_labels)\n",
        "\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "\n",
        "        # statistics\n",
        "        running_loss += loss.item() * local_batch.size(0)\n",
        "        running_corrects += torch.sum(preds == local_labels.data)\n",
        "\n",
        "    epoch_loss = running_loss / len(valid_loader.dataset)\n",
        "    epoch_acc = running_corrects.double() / len(valid_loader.dataset)\n",
        "\n",
        "    print('{} Loss: {:.4f} Acc: {:.4f}'.format('VALIDATION', epoch_loss, epoch_acc))\n",
        "\n",
        "    # deep copy the model\n",
        "    if epoch_acc > best_acc:\n",
        "        best_acc = epoch_acc\n",
        "        #best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    \n",
        "    val_acc_history.append(epoch_acc)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lvli7EkYTILz",
        "colab_type": "text"
      },
      "source": [
        "## PREDICTION/TEST"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5r_9w5YkrxZZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "''' '''\n",
        "# USE THIS IF LOADING A PRE-TRAINED MODEL\n",
        "model = models.resnet50(pretrained=True)\n",
        "#model = ArchNet(num_classes=9)\n",
        "#num_ftrs = model.classifier.in_features\n",
        "#model.classifier = nn.Linear(num_ftrs, num_classes)\n",
        "num_ftrs = model.fc.in_features\n",
        "model.fc = nn.Linear(num_ftrs, num_classes)\n",
        "model.load_state_dict(torch.load('/content/lfi-ws1920-project/models/resnet.pt'))\n",
        "model.to(device)\n",
        "model.eval()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1FB4fGiyB7mP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "correct = 0\n",
        "wrong = 0\n",
        "k = 5 # show probabilities for top k classes\n",
        "# (1) load custom_images folder, which contains 10 images per label\n",
        "for dir in ['custom_images']:\n",
        "\n",
        "    # (2) iterate each style\n",
        "    for style in os.listdir(root_dir + dir):\n",
        "\n",
        "        print('test images for style ', style)\n",
        "        style_folder = os.path.join(root_dir + dir, style)\n",
        "\n",
        "        for img in os.listdir(style_folder):\n",
        "            print('test image:', style_folder + '/' + img)\n",
        "            img_t = Image.open(style_folder + '/' + img).convert('RGB')\n",
        "            batch_t = torch.unsqueeze(train_transform(img_t), 0)\n",
        "\n",
        "            # (3) predict the class\n",
        "            out = model(batch_t.cuda())\n",
        "\n",
        "            #print(torch.nn.functional.softmax(out) * 100)\n",
        "            # CrossEntropy uses log-softmax values -> calculate actual probability values\n",
        "            top5_prob, top5_label = torch.topk(torch.nn.functional.softmax(out, dim=1) * 100, k)\n",
        "            print('Top 5 Probalities: ', top5_prob.tolist())\n",
        "            print('Top 5 Classes: ', top5_label.tolist())\n",
        "            print('------' * 5)\n",
        "\n",
        "            # Now, we need to find out the index where the maximum score in output vector out occurs. We will use this index to find out the prediction.\n",
        "            _, index = torch.max(out, 1)\n",
        "            pred_class = label_encoded[int(index[0])]\n",
        "            print('predicted class =', pred_class)\n",
        "\n",
        "            # (4) test against groundtruth\n",
        "            if pred_class == style:\n",
        "                print('Correclty predicted!')\n",
        "                correct += 1\n",
        "            else:\n",
        "                print('This is wrong!')\n",
        "                wrong += 1\n",
        "            \n",
        "            print('\\n')\n",
        "\n",
        "print('Correct: ', correct)\n",
        "print('Wrong: ', wrong)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c-QMxdzQlQGs",
        "colab_type": "text"
      },
      "source": [
        "# Flask Stuff comes here\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AiCjTaBolhpj",
        "colab_type": "text"
      },
      "source": [
        "## Misc"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LtaiUgUpljp8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_loc = \"/content/archnet.pt\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jvu9h4uqlqUX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "torch.save(model.state_dict(), model_loc)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "piuGIzy3lvFe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "archnet2_point_0 = ArchNet(num_classes=14)\n",
        "archnet2_point_0.load_state_dict(torch.load(model_loc))\n",
        "archnet2_point_0.eval()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "npxaanyQp6fK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install flask-ngrok\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ggtis3atla-h",
        "colab_type": "text"
      },
      "source": [
        "## Server"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bAi_c6G9ldhO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from flask_ngrok import run_with_ngrok\n",
        "from flask import Flask, jsonify, request, Response\n",
        "import cv2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OkVAPuxOmECR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "app = Flask(__name__)\n",
        "run_with_ngrok(app)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "50CvEMLRmG__",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "@app.route('/api/v1/test', methods=['POST'])\n",
        "def get_tasks():\n",
        "    nparr = np.fromstring(request.data, np.uint8)\n",
        "    img = cv2.imdecode(nparr, cv2.IMREAD_COLOR)\n",
        "    response = {'message': 'image received.', 'size':'{}x{}'.format(img.shape[1], img.shape[0])}\n",
        "    return jsonify(response)\n",
        "\n",
        "@app.route('/api/v1/predict', methods=['POST'])\n",
        "def style_prediction():\n",
        "    nparr = np.frombuffer(request.data, np.uint8)\n",
        "    img = cv2.imdecode(nparr, cv2.IMREAD_COLOR)\n",
        "    img_resized = cv2.resize(img, (224, 224)) \n",
        "    img_resized = np.transpose(img_resized,(2,0,1))\n",
        "    img_tensor = torch.from_numpy(img_resized)\n",
        "    img_tensor = img_tensor[None, :, :, :]\n",
        "    img_tensor = img_tensor.type(torch.FloatTensor)\n",
        "    y_pred = model(img_tensor)\n",
        "    _, index = torch.max(y_pred, 1)\n",
        "    predicted_class = label_encoded[int(index[0])]\n",
        "    response = {'message': 'image received.', 'initial_size':'{}x{}'.format(img.shape[1], img.shape[0]), \"prediction\": predicted_class}\n",
        "    return jsonify(response)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I-0iYyipmJxN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "app.run()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "76cw9f8XmM89",
        "colab_type": "text"
      },
      "source": [
        "## Client\n",
        "\n",
        "\n",
        "---\n",
        "Den Teil müsste man sich rauskopieren und dann in einem eigenen notebook oder script ausführen --> oben beim output von app.run() die ngrok url kopieren und beim Aufruf von send_image_to_api den api_baseurl parameter setzen\n",
        "\n",
        "Dann muss man auch ein eigenes Bild schicken\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fWDz-_-nmVBg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import requests"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y7Zc5mpSmWt1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#choose any img from colab, may also use images_csv for that\n",
        "content_type = 'image/jpeg'\n",
        "headers = {'content-type': content_type}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dlg0UdcImOdC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def send_image_to_api(image_location, api_baseurl=\"http://127.0.0.1:5000\", api_endpoint=\"/api/v1/predict\"):\n",
        "    img = open(image_location, 'rb').read()\n",
        "    content_type = 'image/png'\n",
        "    headers = {'content-type': content_type}\n",
        "    targeturl = \"{}{}\".format(api_baseurl, api_endpoint)\n",
        "    return requests.post(targeturl, data=img, headers=headers)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dTUWHsiunFE6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "response = send_image_to_api(images_csv['image_name'][0], api_endpoint = \"/api/v1/test\")\n",
        "response.text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8zvPzliNLGvc",
        "colab_type": "text"
      },
      "source": [
        "# Visualization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-iYDXyRaLO_4",
        "colab_type": "text"
      },
      "source": [
        "## Display Neural Network layer(s)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H2qHj-bP5RQw",
        "colab_type": "text"
      },
      "source": [
        "* https://colab.research.google.com/github/Niranjankumar-c/DeepLearning-PadhAI/blob/master/DeepLearning_Materials/6_VisualizationCNN_Pytorch/CNNVisualisation.ipynb#scrollTo=qv-nJbDFuNuN\n",
        "* https://software.intel.com/en-us/articles/visualising-cnn-models-using-pytorch\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ThbruOn2-Knm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#custom function to conduct occlusion experiments\n",
        "\n",
        "def occlusion(model, image, label, occ_size = 50, occ_stride = 50, occ_pixel = 0.5):\n",
        "  \n",
        "    #get the width and height of the image\n",
        "    width, height = image.shape[-2], image.shape[-1]\n",
        "  \n",
        "    #setting the output image width and height\n",
        "    output_height = int(np.ceil((height-occ_size)/occ_stride))\n",
        "    output_width = int(np.ceil((width-occ_size)/occ_stride))\n",
        "  \n",
        "    #create a white image of sizes we defined\n",
        "    heatmap = torch.zeros((output_height, output_width))\n",
        "    \n",
        "    #iterate all the pixels in each column\n",
        "    for h in range(0, height):\n",
        "        for w in range(0, width):\n",
        "            \n",
        "            h_start = h*occ_stride\n",
        "            w_start = w*occ_stride\n",
        "            h_end = min(height, h_start + occ_size)\n",
        "            w_end = min(width, w_start + occ_size)\n",
        "            \n",
        "            if (w_end) >= width or (h_end) >= height:\n",
        "                continue\n",
        "            \n",
        "            input_image = image.clone().detach()\n",
        "            \n",
        "            #replacing all the pixel information in the image with occ_pixel(grey) in the specified location\n",
        "            input_image[:, :, w_start:w_end, h_start:h_end] = occ_pixel\n",
        "            \n",
        "            #run inference on modified image\n",
        "            output = model(input_image.cuda())\n",
        "            output = nn.functional.softmax(output, dim=1)\n",
        "            prob = output.tolist()[0][label]\n",
        "            \n",
        "            #setting the heatmap location to probability value\n",
        "            heatmap[h, w] = prob \n",
        "\n",
        "    return heatmap"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DaZPrButCc0Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#images,classes = next(iter(training_generator))\n",
        "#run the model on the images\n",
        "outputs = model(inputs.cuda())\n",
        "\n",
        "#get the maximum class \n",
        "labels, pred = torch.max(outputs.data, 1)\n",
        "\n",
        "#running inference on the images without occlusion\n",
        "\n",
        "#vgg16 pretrained model\n",
        "\n",
        "outputs = model(inputs.cuda())\n",
        "print(outputs.shape)\n",
        "\n",
        "#passing the outputs through softmax to interpret them as probability\n",
        "outputs = nn.functional.softmax(outputs, dim = 1)\n",
        "\n",
        "#getting the maximum predicted label\n",
        "prob_no_occ, pred = torch.max(outputs.data, 1)\n",
        "\n",
        "#get the first item\n",
        "prob_no_occ = prob_no_occ[0].item()\n",
        "\n",
        "print(prob_no_occ)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bgMbzOBZ-M7F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "heatmap = occlusion(model, inputs, pred[0].item(), 32, 14)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UXe0rs3OBx_g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "imgplot = sns.heatmap(heatmap, xticklabels=False, yticklabels=False, vmax=prob_no_occ)\n",
        "figure = imgplot.get_figure()    \n",
        "#figure.savefig('svm_conf.png', dpi=400)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xvLQrvVBDjPa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Make a grid from batch\n",
        "out = torchvision.utils.make_grid(inputs)\n",
        "imshow(out, title=[label_encoded[int(x)] for x in classes])"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}